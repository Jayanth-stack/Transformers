{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.21.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.4/10.2 MB 20.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.2 MB 23.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 34.5 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, safetensors, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 huggingface-hub-0.29.3 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jayan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jayan\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 477352.37 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1419296.60 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 766604.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print (train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jayan\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train dataset: 2303695\n",
      "Number of tokens in validation dataset: 238656\n",
      "Number of tokens in test dataset: 273178\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to count tokens in a dataset\n",
    "def count_tokens(dataset):\n",
    "    total_tokens = 0\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer.tokenize(example['text'])\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "# Count tokens for each dataset\n",
    "train_tokens = count_tokens(train_dataset)\n",
    "val_tokens = count_tokens(val_dataset)\n",
    "test_tokens = count_tokens(test_dataset)\n",
    "\n",
    "print(f\"Number of tokens in train dataset: {train_tokens}\")\n",
    "print(f\"Number of tokens in validation dataset: {val_tokens}\")\n",
    "print(f\"Number of tokens in test dataset: {test_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to tokenize text and build vocabulary\n",
    "def build_vocabulary(dataset, tokenizer, vocab_size=10000, min_freq=5):\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    # Tokenize the dataset and count token frequencies\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer.tokenize(example['text'])\n",
    "        token_counter.update(tokens)\n",
    "    \n",
    "    # Filter out rare tokens\n",
    "    vocab = [token for token, freq in token_counter.items() if freq >= min_freq]\n",
    "    \n",
    "    # Keep the top `vocab_size` most frequent tokens\n",
    "    vocab = vocab[:vocab_size]\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "    vocab = special_tokens + vocab\n",
    "    \n",
    "    # Create token to index mapping\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    \n",
    "    return token_to_idx\n",
    "\n",
    "# Build vocabulary for the training dataset\n",
    "vocab = build_vocabulary(train_dataset, tokenizer)\n",
    "\n",
    "# Function to convert tokens to indices\n",
    "def tokens_to_indices(tokens, token_to_idx):\n",
    "    return [token_to_idx.get(token, token_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# Function to create training examples\n",
    "def create_training_examples(dataset, tokenizer, token_to_idx, seq_len=8):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        tokens = tokenizer.tokenize(example['text'])\n",
    "        token_indices = tokens_to_indices(tokens, token_to_idx)\n",
    "        \n",
    "        for i in range(len(token_indices) - seq_len):\n",
    "            inputs.append(token_indices[i:i+seq_len])\n",
    "            targets.append(token_indices[i+seq_len])\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "# Create training examples for the training dataset\n",
    "train_inputs, train_targets = create_training_examples(train_dataset, tokenizer, vocab)\n",
    "\n",
    "# Function to split data into batches\n",
    "def create_batches(inputs, targets, batch_size=64):\n",
    "    batches = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch_inputs = inputs[i:i+batch_size]\n",
    "        batch_targets = targets[i:i+batch_size]\n",
    "        batches.append((batch_inputs, batch_targets))\n",
    "    return batches\n",
    "\n",
    "# Create batches for the training dataset\n",
    "train_batches = create_batches(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.21074390411377\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dims=[256, 128]):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.output = nn.Linear(hidden_dims[1], vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input tokens\n",
    "        x = self.embedding(x)\n",
    "        # Combine embeddings by averaging\n",
    "        x = x.mean(dim=1)\n",
    "        # Pass through feed-forward layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        # Apply softmax to get probability distribution\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dims = [256, 128]\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = FeedForwardNN(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dims=hidden_dims)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Example of how to use the model with a batch of inputs\n",
    "batch_inputs, batch_targets = train_batches[0]\n",
    "batch_inputs = torch.tensor(batch_inputs)\n",
    "batch_targets = torch.tensor(batch_targets)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(batch_inputs)\n",
    "loss = criterion(outputs, batch_targets)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m batch_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_targets)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m batch_targets \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch_size)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Then flatten both for CrossEntropy:\u001b[39;00m\n\u001b[0;32m     59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))         \u001b[38;5;66;03m# (seq_len * batch_size, vocab_size)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, nhead=4, nhid=512, nlayers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "nhead = 4\n",
    "nhid = 512\n",
    "nlayers = 2\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(vocab_size=len(vocab), embedding_dim=embedding_dim, nhead=nhead, nhid=nhid, nlayers=nlayers, dropout=dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Example of how to use the model with a batch of inputs\n",
    "batch_inputs, batch_targets = train_batches[0]\n",
    "batch_inputs = torch.tensor(batch_inputs).transpose(0, 1)  # Transpose for transformer input shape\n",
    "batch_targets = torch.tensor(batch_targets)\n",
    "\n",
    "# Forward pass\n",
    "batch_targets = batch_targets.transpose(0, 1)  # (seq_len, batch_size)\n",
    "\n",
    "# Then flatten both for CrossEntropy:\n",
    "outputs = outputs.view(-1, outputs.size(-1))         # (seq_len * batch_size, vocab_size)\n",
    "batch_targets = batch_targets.contiguous().view(-1)  # (seq_len * batch_size,)\n",
    "\n",
    "loss = criterion(outputs, batch_targets)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
